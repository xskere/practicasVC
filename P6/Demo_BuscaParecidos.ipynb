{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T15:23:05.770986700Z",
     "start_time": "2023-12-06T15:23:05.731204700Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import FaceNormalizationUtils as faceutils\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "# mode and so on\n",
    "from collections import Counter\n",
    "\n",
    "#MTCNN face detector\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "#deepface\n",
    "from deepface import DeepFace\n",
    "from deepface.commons import functions\n",
    "\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T15:23:05.780327900Z",
     "start_time": "2023-12-06T15:23:05.755792300Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_embs(X, batch_size=2):\n",
    "    norm_images = prewhiten(X)\n",
    "    pd = []\n",
    "    for start in range(0, len(norm_images), batch_size):\n",
    "        pd.append(model.predict_on_batch(norm_images[start:start+batch_size]))\n",
    "    return l2_normalize(np.concatenate(pd))\n",
    "\n",
    "def l2_normalize(x, axis=-1, epsilon=1e-10):\n",
    "    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n",
    "    return output\n",
    "\n",
    "def prewhiten(x):\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y\n",
    "\n",
    "# Designed for UTKFace as filename contains biometric information\n",
    "def TrainEmbedingsUTKFace(folders,outputfile):\n",
    "    nimgs = 0\n",
    "    Xorig = []\n",
    "    X = []\n",
    "    Gender = []\n",
    "    Age = []\n",
    "    Etnia = []\n",
    "    \n",
    "    for directory in folders:\n",
    "        #print(directory)\n",
    "        for path, subdirs, files in os.walk(directory):\n",
    "            for name in files:\n",
    "                #print(name)\n",
    "                if name.endswith(\".jpg\") and nimgs < 2000:\n",
    "                #if name.endswith(\".png\"): # and nimgs < 3000:\n",
    "                    img_path = os.path.join(path, name)\n",
    "                    #print(img_path)\n",
    "\n",
    "                    image = cv2.imread(img_path)\n",
    "\n",
    "                    if (type(image) is np.ndarray):\n",
    "                        #print(img_path)\n",
    "                        # Search face \n",
    "                        values = DetectLargestFaceEyesMTCNN(image)\n",
    "                        if values is not None:\n",
    "                            #print(nimgs)\n",
    "                            face, eyes, shape = values\n",
    "\n",
    "                            # draws container\n",
    "                            [x, y, w, h] = face\n",
    "                            if x > -1:\n",
    "                                # Eyes\n",
    "                                [lex, ley, rex, rey] = eyes\n",
    "                                if lex > -1:\n",
    "                                    \n",
    "                                    B, G, R = cv2.split(image)\n",
    "\n",
    "                                    # Normalize for Facenet\n",
    "                                    normalizatorHS.normalize_gray_img(B, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                                    Bnorm = normalizatorHS.normf_image\n",
    "                                    normalizatorHS.normalize_gray_img(G, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                                    Gnorm = normalizatorHS.normf_image\n",
    "                                    normalizatorHS.normalize_gray_img(R, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                                    Rnorm = normalizatorHS.normf_image\n",
    "                                    NormBGRHS = cv2.merge((Bnorm, Gnorm, Rnorm))\n",
    "                                    #cv2.imshow(\"Normalized\", NormBGRHS)\n",
    "\n",
    "                                    # Cropping from HS for facenet\n",
    "                                    # Usa nyoki https://github.com/nyoki-mtl/keras-facenet\n",
    "                                    NormBGR = NormBGRHS[35:115, 39:119, :]\n",
    "\n",
    "                                    # SOft biometric data are extracted from filename\n",
    "                                    if NormBGR is not None:\n",
    "                                        nimgs = nimgs + 1\n",
    "                                        #print(nimgs)\n",
    "\n",
    "                                        fsub = name.find('_')\n",
    "                                        sage = name[:fsub]\n",
    "                                        #print(sage)\n",
    "                                        sub = name[fsub + 1:]\n",
    "                                        fsub = sub.find('_')\n",
    "                                        sgender = sub[:fsub]\n",
    "                                        sub = sub[fsub + 1:]\n",
    "                                        fsub = sub.find('_')\n",
    "                                        setnia = sub[:fsub]\n",
    "\n",
    "                                        # print(name)\n",
    "                                        # print(sage)\n",
    "                                        # print(sgender)\n",
    "                                        # print(setnia)\n",
    "                                        # print(nimgs)\n",
    "\n",
    "                                        # Facenet kearas expects 160x160\n",
    "                                        #imaged = cv2.resize(NormBGR, (160, 160))\n",
    "                                        \n",
    "                                        # Obtiene embeddings\n",
    "                                        imaged = cv2.resize(NormBGR, dim, interpolation = cv2.INTER_AREA)\n",
    "                                        \n",
    "                                        # Mantengo originales para mostrar parecido\n",
    "                                        imager = cv2.resize(image, (200, 200), interpolation=cv2.INTER_AREA)\n",
    "                                        Xorig.append(imager)\n",
    "                                        # Facenet\n",
    "                                        X.append(imaged)\n",
    "                                        Gender.append(sgender)\n",
    "                                        Age.append(sage)\n",
    "                                        Etnia.append(setnia)\n",
    "    \n",
    "    if nimgs > 0:\n",
    "        # Compute embeddings \n",
    "        embs = calc_embs(np.array(X))\n",
    "\n",
    "        fid = open(outputfile, \"wb\")\n",
    "\n",
    "        pickle.dump([nimgs, X, embs, Age, Gender, Etnia], fid)\n",
    "        fid.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T15:23:05.781508700Z",
     "start_time": "2023-12-06T15:23:05.773985400Z"
    }
   },
   "outputs": [],
   "source": [
    "def GetSimilarNN(X,idxsimilar, nbrs, embs, Title):\n",
    "    # Gets nearest neighbors\n",
    "    distance, indices = nbrs.kneighbors(embs)\n",
    "    minp = indices[0][0]\n",
    "\n",
    "    # Keeps copy of last grames NN\n",
    "    idxsimilar.extend(indices[0])\n",
    "\n",
    "    # Remove old ones when more than nframes accumulated\n",
    "    if len(idxsimilar) > kvecinos * nframeskvecinos:\n",
    "        idxsimilar = idxsimilar[kvecinos:]\n",
    "\n",
    "    # print('Lista con ' + str(len(idxsimilar)) + ' ' + str(idxsimilar) + '\\n')\n",
    "\n",
    "    # enough history\n",
    "    if len(idxsimilar) > kvecinos:\n",
    "        minp = GetClosesetMode(idxsimilar)\n",
    "\n",
    "    # Larger for visualization\n",
    "    imageS = cv2.resize(X[minp], (320, 320))\n",
    "\n",
    "    # Soft biometrics labels\n",
    "    if len(idxsimilar) > 0:\n",
    "        genderAux = []\n",
    "        etniaAux = []\n",
    "        for i in idxsimilar:\n",
    "            try:\n",
    "                etniaAux.append(int(Etnia[int(i)]))\n",
    "                genderAux.append(int(Gender[int(i)]))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        modesG = stats.mode(genderAux)\n",
    "        modesE = stats.mode(etniaAux)\n",
    "        edad = np.array([int(Age[int(i)]) for i in idxsimilar]).mean()\n",
    "\n",
    "        if modesG[0] == 0:\n",
    "            gen = 'M'\n",
    "        else:\n",
    "            gen = 'F'\n",
    "\n",
    "        if modesE[0] == 0:\n",
    "            et = 'B'\n",
    "        elif modesE[0] == 1:\n",
    "            et = 'N'\n",
    "        elif modesE[0] == 2:\n",
    "            et = 'A'\n",
    "        elif modesE[0] == 3:\n",
    "            et = 'H'\n",
    "        else:\n",
    "            et = 'L'\n",
    "\n",
    "        cv2.putText(imageS, '%s %s %d (%s)' % (gen, et, int(edad), Age[minp]), (10, 30), font, 0.5, (255, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(Title, imageS)\n",
    "\n",
    "    return idxsimilar\n",
    "\n",
    "def GetSimilarNNAge(X,idxsimilar, nbrs, embs, Title, age):\n",
    "    # Gets nearest neighbors\n",
    "    distance, indices = nbrs.kneighbors(embs)\n",
    "    # minp = indices[0][0]\n",
    "    ages = []\n",
    "    for i in indices[0]:\n",
    "        ages.append(int(Age[i]))\n",
    "        # print(Age[i])\n",
    "    indProxAge, proxAge = min(enumerate(ages), key=lambda x: abs(x[1] - age))\n",
    "    print(indices[0][indProxAge], proxAge)\n",
    "    minp = indices[0][indProxAge]\n",
    "    print(minp)\n",
    "\n",
    "    # Keeps copy of last grames NN\n",
    "    idxsimilar.extend(indices[0])\n",
    "    idxsimilar = list(set(idxsimilar))\n",
    "    # idxsimilar.extend(indices[0])\n",
    "\n",
    "    # Remove old ones when more than nframes accumulated\n",
    "    if len(idxsimilar) > kvecinos * nframeskvecinos:\n",
    "        idxsimilar = idxsimilar[kvecinos:]\n",
    "\n",
    "    # print('Lista con ' + str(len(idxsimilar)) + ' ' + str(idxsimilar) + '\\n')\n",
    "\n",
    "    # Larger for visualization\n",
    "    imageS = cv2.resize(X[minp], (320, 320))\n",
    "\n",
    "    # Soft biometrics labels\n",
    "    if len(idxsimilar) > 0:\n",
    "        genderAux = []\n",
    "        etniaAux = []\n",
    "        for i in idxsimilar:\n",
    "            try:\n",
    "                etniaAux.append(int(Etnia[int(i)]))\n",
    "                genderAux.append(int(Gender[int(i)]))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        modesG = stats.mode(genderAux)\n",
    "        modesE = stats.mode(etniaAux)\n",
    "        edad = np.array([int(Age[int(i)]) for i in idxsimilar]).mean()\n",
    "\n",
    "        if modesG[0] == 0:\n",
    "            gen = 'M'\n",
    "        else:\n",
    "            gen = 'F'\n",
    "\n",
    "        if modesE[0] == 0:\n",
    "            et = 'B'\n",
    "        elif modesE[0] == 1:\n",
    "            et = 'N'\n",
    "        elif modesE[0] == 2:\n",
    "            et = 'A'\n",
    "        elif modesE[0] == 3:\n",
    "            et = 'H'\n",
    "        else:\n",
    "            et = 'L'\n",
    "\n",
    "        cv2.putText(imageS, '%s %s %d (%s)' % (gen, et, int(edad), Age[minp]), (10, 30), font, 0.5, (255, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(Title, imageS)\n",
    "\n",
    "    return idxsimilar\n",
    "\n",
    "def GetSimilarNNOpositeGender(X,idxsimilar, nbrs, embs, Title):\n",
    "    # Gets nearest neighbors\n",
    "    distance, indices = nbrs.kneighbors(embs)\n",
    "    # minp = indices[0][0]\n",
    "    if idxsimilar == None:\n",
    "        idxsimilar = []\n",
    "\n",
    "    # Keeps copy of last grames NN\n",
    "    # idxsimilar.extend(indices[0])\n",
    "    idxsimilar.extend(indices[0])\n",
    "    idxsimilar = list(set(idxsimilar))\n",
    "\n",
    "    # Remove old ones when more than nframes accumulated\n",
    "    if len(idxsimilar) > kvecinos * nframeskvecinos:\n",
    "        idxsimilar = idxsimilar[kvecinos:]\n",
    "\n",
    "    # if len(idxsimilar) > kvecinos:\n",
    "    #     minp = GetClosesetMode(idxsimilar)\n",
    "\n",
    "    # print('Lista con ' + str(len(idxsimilar)) + ' ' + str(idxsimilar) + '\\n')\n",
    "\n",
    "    # Larger for visualization\n",
    "\n",
    "    # Soft biometrics labels\n",
    "    if len(idxsimilar) > 0:\n",
    "        genderAux = []\n",
    "        etniaAux = []\n",
    "        for i in idxsimilar:\n",
    "            try:\n",
    "                etniaAux.append(int(Etnia[int(i)]))\n",
    "                genderAux.append(int(Gender[int(i)]))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        modesG = stats.mode(genderAux)\n",
    "        modesE = stats.mode(etniaAux)\n",
    "        edad = np.array([int(Age[int(i)]) for i in idxsimilar]).mean()\n",
    "\n",
    "        if modesG[0] == 0:\n",
    "            genre = 1\n",
    "            gen = 'M'\n",
    "        else:\n",
    "            genre = 0\n",
    "            gen = 'F'\n",
    "\n",
    "        minp = None\n",
    "        for i in indices[0]:\n",
    "            if int(Gender[i]) == genre:\n",
    "                minp = i\n",
    "            \n",
    "        if minp == None:\n",
    "            print(\"No hay similitudes con este género\")\n",
    "            return\n",
    "        \n",
    "        imageS = cv2.resize(X[minp], (320, 320))\n",
    "\n",
    "        if modesE[0] == 0:\n",
    "            et = 'B'\n",
    "        elif modesE[0] == 1:\n",
    "            et = 'N'\n",
    "        elif modesE[0] == 2:\n",
    "            et = 'A'\n",
    "        elif modesE[0] == 3:\n",
    "            et = 'H'\n",
    "        else:\n",
    "            et = 'L'\n",
    "\n",
    "        cv2.putText(imageS, '%s %s %d (%s)' % (gen, et, int(edad), Age[minp]), (10, 30), font, 0.5, (255, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(Title, imageS)\n",
    "\n",
    "    return idxsimilar\n",
    "\n",
    "def getLargestMTCNNBB(objects):\n",
    "        if len(objects) < 1:\n",
    "            return -1\n",
    "        elif len(objects) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            areas = [ (det['box'][2]*det['box'][3]) for det in objects ]\n",
    "            return np.argmax(areas)\n",
    "        \n",
    "def DetectLargestFaceEyesMTCNN(img):\n",
    "    global detectormtcnn\n",
    "    \n",
    "    results = detectormtcnn.detect_faces(img)\n",
    "\n",
    "    if not results is None:\n",
    "        index = getLargestMTCNNBB(results)\n",
    "\n",
    "        if len(results) < 1:\n",
    "            return None\n",
    "\n",
    "        # laergest face\n",
    "        face_info = results[index]\n",
    "\n",
    "        #print(face_info)\n",
    "\n",
    "        [x, y, w, h] = face_info['box']\n",
    "        le = face_info['keypoints']['left_eye']\n",
    "        re = face_info['keypoints']['right_eye']\n",
    "\n",
    "        return [x,y,w,h], [le[0], le[1], re[0], re[1]], [face_info['keypoints']['left_eye'], face_info['keypoints']['right_eye'],\n",
    "                      face_info['keypoints']['nose'], face_info['keypoints']['mouth_left'],\n",
    "                      face_info['keypoints']['mouth_right']]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def ResetDetectionCounters():\n",
    "    global nconsecutivenodetected, nconsecutivedetected, idxsimilarFN\n",
    "\n",
    "    nconsecutivenodetected = nconsecutivenodetected + 1\n",
    "    if nconsecutivenodetected > 10:\n",
    "        nconsecutivedetected = 0\n",
    "        idxsimilarFN = []\n",
    "\n",
    "def GetClosesetMode(list):\n",
    "    # Get occurrences list\n",
    "    occ = Counter(list)\n",
    "\n",
    "    # Get the mode, if multiple modes present, gets the clostest observing position among beigbors\n",
    "    prima = 0\n",
    "    maxpun = 0\n",
    "    for neighbor, count in occ.most_common(10):\n",
    "        if prima == 0:\n",
    "            prima = 1\n",
    "            mode = count\n",
    "        else:\n",
    "            if count < mode:\n",
    "                break\n",
    "\n",
    "        #print('%s: %7d' % (neighbor, count))\n",
    "\n",
    "        # using enumerate()\n",
    "        # to find indices for 3\n",
    "        neighbor_pos = [i for i, value in enumerate(list) if value == neighbor]\n",
    "        # printing resultant list\n",
    "        #print(\"New indices list : \" + str(neighbor_pos))\n",
    "\n",
    "        pun = 0\n",
    "        for pos in neighbor_pos:\n",
    "            pun = pun + kvecinos - (pos % kvecinos)\n",
    "        #print(pun)\n",
    "        if pun > maxpun:\n",
    "            maxpun = pun\n",
    "            closest_neighbor = neighbor\n",
    "\n",
    "    return closest_neighbor\n",
    "\n",
    "def changeSign(lst):\n",
    "    return [-i for i in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T15:24:12.414767Z",
     "start_time": "2023-12-06T15:23:05.780327900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model\n",
      "Camera 2\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    }
   ],
   "source": [
    "# Face detector\n",
    "detectormtcnn = MTCNN()\n",
    "# Normalization utilities\n",
    "normalizatorHS = faceutils.Normalization()\n",
    "\n",
    "# Embeddings deepface\n",
    "model = DeepFace.build_model(\"Facenet\")\n",
    "target_size = model.layers[0].input_shape\n",
    "dim = (int(target_size[0][1]), int(target_size[0][2]))\n",
    "\n",
    "# 1 to create a new appearance dataset\n",
    "creadataset = 0\n",
    "if creadataset == 1:\n",
    "    print(\"Create model\")\n",
    "    #Dataset folders\n",
    "    folders = [\"Datasets/part1/part1\", \"Datasets/part2/part2\",\n",
    "               \"Datasets/part3/part3\"]\n",
    "    # TrainEmbedingsUTKFace(folders,\"D:/FACEScode_models/UTKFace_DLIB_Nyoki_deepfaceSP2021.obj\")\n",
    "\n",
    "#Load appearance dataset\n",
    "print(\"Reading model\")\n",
    "#MODIFICAR EN BASE A TU RUTA\n",
    "fid = open(\"UTKFace_DLIB_Nyoki.obj\", \"rb\") #Modelo reducido\n",
    "#fid = open(\"E:/FACEScode_models/UTKFace_DLIB_Nyoki.obj\", \"rb\") # Entrenado similar a normalización Nyoki con 20k\n",
    "#fid = open(\"D:/FACEScode_models/UTKFace_DLIB_Nyoki_deepfaceSP2021.obj\", \"rb\") # Entrenado similar a normalización Nyoki con 2k\n",
    "nimgs, X_FN, embsFN, Age, Gender, Etnia = pickle.load(fid)\n",
    "#print(embsFN.shape)\n",
    "\n",
    "# Tree for KNN search\n",
    "kvecinos = 15\n",
    "nframeskvecinos = 5\n",
    "nbrsFN = NearestNeighbors(n_neighbors=kvecinos).fit(embsFN)\n",
    "\n",
    "# Fonts\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Webcam connection, check unitl one is located\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Check for other cameras\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(2)\n",
    "        if not cap.isOpened():\n",
    "            print('Camera error')\n",
    "            exit(0)\n",
    "        else:\n",
    "            print('Camera 2')\n",
    "    else:\n",
    "        print('Camera 1')\n",
    "else:\n",
    "    print('Camera 2')\n",
    "    \n",
    "#Set camera resolution\n",
    "cap.set(3,640);\n",
    "cap.set(4,480);\n",
    "\n",
    "# Initializations\n",
    "debug = 0\n",
    "nconsecutivedetected = 0\n",
    "nconsecutivenodetected = 0\n",
    "idxsimilarFN = []\n",
    "\n",
    "while True:\n",
    "    # Get frame\n",
    "    t = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    # For HS normalization split channels\n",
    "    B, G, R = cv2.split(frame)\n",
    "\n",
    "    # Search face \n",
    "    values = DetectLargestFaceEyesMTCNN(frame)\n",
    "    if values is not None:\n",
    "        face, eyes, shape = values\n",
    "\n",
    "        #draws face container\n",
    "        [x, y , w, h] = face\n",
    "        if x > -1:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "            # draws eyes and mask if available\n",
    "            [lex, ley, rex, rey] = eyes\n",
    "            if lex > -1:                \n",
    "                nconsecutivedetected = nconsecutivedetected + 1  #11?\n",
    "                nconsecutivenodetected = 0\n",
    "                    \n",
    "                # Show detected facial elements\n",
    "                for (x, y) in shape:\n",
    "                    cv2.circle(frame, (x, y), 2, (255, 255, 255), -1)\n",
    "                cv2.circle(frame, ((int)(lex), (int)(ley)), 4, (0, 0, 255), -1)\n",
    "                cv2.circle(frame, ((int)(rex), (int)(rey)), 4, (0, 255, 0), -1)\n",
    "\n",
    "                # HS normalization for facenet\n",
    "                normalizatorHS.normalize_gray_img(B, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Bnorm = normalizatorHS.normf_image\n",
    "                normalizatorHS.normalize_gray_img(G, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Gnorm = normalizatorHS.normf_image\n",
    "                normalizatorHS.normalize_gray_img(R, lex, ley, rex, rey, faceutils.Kind_wraping.HS)\n",
    "                Rnorm = normalizatorHS.normf_image\n",
    "                NormBGRHS = cv2.merge((Bnorm, Gnorm, Rnorm))\n",
    "                # cv2.imshow(\"Normalized\", NormBGRHS)\n",
    "                    \n",
    "                # Cropping from HS for facenet\n",
    "                # De HS a lo proximadamente usa nyok https://github.com/nyoki-mtl/keras-facenet\n",
    "                NormBGR = NormBGRHS[35:115, 39:119, :]\n",
    "                # cv2.imshow(\"Normalizedc\", NormBGR)\n",
    "                 \n",
    "                # Obtiene embeddings\n",
    "                img1 = cv2.resize(NormBGR, dim, interpolation = cv2.INTER_AREA)\n",
    "                cv2.imshow(\"\", img1)\n",
    "                #embs = model.predict(img1[None,...])\n",
    "                embs = calc_embs(np.array([img1]))\n",
    "                embs = changeSign(embs)\n",
    "                # idxsimilarFN = GetSimilarNNAge(X_FN,idxsimilarFN, nbrsFN, embs, 'FN Me recuerdas a ...', 60)\n",
    "                idxsimilarFN = GetSimilarNNOpositeGender(X_FN,idxsimilarFN, nbrsFN, embs, 'FN Me recuerdas a ...')\n",
    "                \n",
    "            else:\n",
    "                ResetDetectionCounters()\n",
    "        else:\n",
    "            ResetDetectionCounters()\n",
    "    else:\n",
    "        ResetDetectionCounters()\n",
    "\n",
    "    if debug:\n",
    "        print(\"Processing time : {:.3f}\".format(time.time() - t))\n",
    "\n",
    "    # Show resulting image\n",
    "    cv2.imshow('Cam', frame)\n",
    "    \n",
    "    # Esc to finish\n",
    "    tec = cv2.waitKey(5)\n",
    "    if tec & tec == 27:  # Esc\n",
    "        break  \n",
    "\n",
    "# Close windows and release camera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "fid = open(\"UTKFace_DLIB_Nyoki.obj\", \"rb\") #Modelo reducido\n",
    "#fid = open(\"E:/FACEScode_models/UTKFace_DLIB_Nyoki.obj\", \"rb\") # Entrenado similar a normalización Nyoki con 20k\n",
    "#fid = open(\"D:/FACEScode_models/UTKFace_DLIB_Nyoki_deepfaceSP2021.obj\", \"rb\") # Entrenado similar a normalización Nyoki con 2k\n",
    "nimgs, X_FN, embsFN, Age, Gender, Etnia = pickle.load(fid)\n",
    "\n",
    "minp = 7819\n",
    "imageS = cv2.resize(X_FN[minp], (320, 320))\n",
    "print(Gender[minp])\n",
    "cv2.imshow(\"\", imageS)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('deepface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "12028effb1af0cd2244438ff9b17d06bb1d7695ec7a554a144e43ec4b8b79006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
